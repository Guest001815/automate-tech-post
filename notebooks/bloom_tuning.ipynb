{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install requirements\n",
        "First, run the cells below to install the requirements:"
      ],
      "metadata": {
        "id": "mfkbe2bfh3V_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV6KLNQNhc96"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model loading\n",
        "Let's load the bloomz-3B model!\n",
        "\n",
        "We're also going to load the bigscience/bloomz-3b which is the tokenizer for all of the BLOOM models.\n",
        "\n",
        "This step will take some time, as we have to download the model weights which are >6GB."
      ],
      "metadata": {
        "id": "BoVD8UZ-iEdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "veqeC3c4he0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloomz-3b\", \n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True, \n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")"
      ],
      "metadata": {
        "id": "OetgNYg9iK8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "It's important to observe the model's construction so you can ensure you know which modules you should apply LoRA to.\n",
        "\n",
        "As per the paper, we're going to focus on the attention weights - so keep an eye out for modules like: q_proj, v_proj, query_key_value. This is model dependent."
      ],
      "metadata": {
        "id": "CE1feb1HiPI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "XMxJtzJsiP6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-processing on the model\n",
        "Finally, we need to apply some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in float32 for stability. We also cast the output of the last layer in float32 for the same reasons."
      ],
      "metadata": {
        "id": "Fdl4H04hiYmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "P2X1d2H8iZYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply LoRA\n",
        "Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function from peft.\n",
        "\n",
        "## Helper Function to Print Parameter %age\n",
        "This is just a helper function to print out just how much LoRA reduces the number of trainable parameters."
      ],
      "metadata": {
        "id": "R2lMXfGKicFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "T1c_D3yUifvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing LoRA Config\n",
        "There's a lot to unpack here - so let's talk about the main parameters:\n",
        "\n",
        "`r:` is the \"rank\" of the two decomposed matrices we'll be using to represent our weight matrix. In reality, this is the dimension of the decomposed matrices.\n",
        "\n",
        "`target_modules: `As LoRA can be applied to any weight matrix - we need to configure which module (weight matrix) it's being applied to. The paper suggests applying it to the Attention weights, and so we're doing that here. Be mindful that, while BLOOMZ's attention weight modules are called query_key_value - other models will name them with different convention. Please ensure you look at your model's architecture and select the appropriate module.\n",
        "\n",
        "`task_type:` This is a derived property. If you're using a causal model, this should be set to CAUSAL_LM. Please ensure this property is set based on your selected model.\n",
        "\n",
        "Again, while this is the way we're leveraging LoRA in this notebook - it can be used in conjunction with many different models - and many different tasks. You can even use it for tasks like [token classification](https://huggingface.co/docs/peft/task_guides/token-classification-lora)!"
      ],
      "metadata": {
        "id": "PZz-bL_5iiKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model \n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "HtbWvR3HilsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "We can simply load our dataset from ðŸ¤— Hugging Face with the load_dataset method!"
      ],
      "metadata": {
        "id": "pNbD6SITiy6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Y_I_Go9HjsZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "HUGGING_FACE_USER_NAME = \"your-user-name\"\n",
        "dataset_name = \"your-data-set\"\n",
        "dataset_name = f\"{user_name}/{dataset_name}\"\n",
        "product_name = \"product\"\n",
        "product_desc = \"description\"\n",
        "product_ad = \"automating_tech_blog\""
      ],
      "metadata": {
        "id": "BzeMtG4-ixyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name)\n",
        "print(dataset)\n",
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "QUFjajBxi78h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we're using BLOOMZ (which is an instruct-tuned base model), we should see better results providing the instruction - though that is not necessary."
      ],
      "metadata": {
        "id": "-iWvtV33jKjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(summary: str, post: str) -> str:\n",
        "  prompt = f\"### INSTRUCTION\\nBelow is a summary of a post and its corresponding social media post, please write social media post for this blog.\\n\\n### Summary:\\n{summary}\\n### Post:\\n{post}\\n\"\n",
        "  return prompt\n",
        "\n",
        "mapped_dataset = dataset.map(lambda samples: tokenizer(generate_prompt(samples['summary'], samples['post'])))\n"
      ],
      "metadata": {
        "id": "aKvl5FdljLF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Trainer class contains all the usual suspects - these are the same hyper-parameters you know and love from traditional ML applications!\n",
        "\n",
        "If you're running into CUDA memory issues - please modify both the per_device_train_batch_size to be lower, and also reduce r in your LoRAConfig. You'll need to restart and re-run your notebook after doing so."
      ],
      "metadata": {
        "id": "FZ3zUb03jj8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model, \n",
        "    train_dataset=mapped_dataset[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=6, \n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        max_steps=100, \n",
        "        learning_rate=1e-3, \n",
        "        fp16=True,\n",
        "        logging_steps=1, \n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "3Z8bpXhmjkP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Share adapters on the ðŸ¤— Hub\n",
        "Normally, we would only seek to push the LoRA adapters to the hub. This is a lightweight and memory efficient way to push this model - as you can pull the base model down as part of the inference pipeline.\n",
        "\n",
        "However, if you want to leverage the one-click-deploy features of Hugging Face, you'll need to first merge_and_unload() the model and push the resulting model to the hub. This process will merge the LoRA weights back into the base model.\n",
        "\n",
        "Please note that if you leveraged bitsandbytes to load the model in 8-bit - you cannot merge the weights into the base model at this step."
      ],
      "metadata": {
        "id": "H5cZqZ5RjnAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"YOUR MODEL NAME HERE\"\n",
        "\n",
        "model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)\n"
      ],
      "metadata": {
        "id": "2-DrsujyjpHL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}